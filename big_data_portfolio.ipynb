{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Uso, procesamiento y visualización de grandes volúmenes de datos**"
      ],
      "metadata": {
        "id": "DNepAse-BTdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuración de entorno de trabajo en Colab para utilizar PySpark**"
      ],
      "metadata": {
        "id": "WBfspcjABwkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "id": "NhWLVLQI-hQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b735d50f-d9a3-4227-c1e7-87507c543538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=7db22affb3fdfa17e89faad62aa49d2b6f08ffbb25a52acfa44c66b8ac8a7699\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero se crea un contexto de Spark, el cual es necesario para interactuar con el clúster de Spark y ejecutar las operaciones de procesamiento distribuido."
      ],
      "metadata": {
        "id": "CELlDi-SkU6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.mllib.util import MLUtils\n",
        "\n",
        "sc = SparkContext(appName=\"BigData\")"
      ],
      "metadata": {
        "id": "CeZpHUhY-fzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Modelo de regresión**\n",
        "#### Decision Tree"
      ],
      "metadata": {
        "id": "u_2ZNs_hDBNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel"
      ],
      "metadata": {
        "id": "5fr-S0DT60o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección, se carga un archivo de datos y se almacena en un Resilient Distributed Dataset (RDD). Cada registro del RDD es un LabeledPoint, que contiene una etiqueta y un conjunto de características. Los datos se dividen en conjuntos de entrenamiento y prueba, después se entrena un modelo de regresión de árbol de decisión utilizando los datos de entrenamiento.\n",
        "\n",
        "Se realizan predicciones utilizando el modelo entrenado en el conjunto de prueba, y se calcula el Error Cuadrático Medio (MSE) para evaluar el rendimiento del modelo."
      ],
      "metadata": {
        "id": "r_zYSfuzkm8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Carga y analisis del archivo de datos en un RDD de LabeledPoint\n",
        "    data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
        "\n",
        "    # Division de los datos en conjuntos de entrenamiento y prueba\n",
        "    (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "    # Entrenamiento del modelo\n",
        "    model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo={},\n",
        "                                        impurity='variance', maxDepth=5, maxBins=32)\n",
        "\n",
        "    # Evaluacion del modelo en instancias de prueba y calculo del error de prueba\n",
        "    predictions = model.predict(testData.map(lambda x: x.features))\n",
        "    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
        "    testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
        "                                      float(testData.count())\n",
        "\n",
        "    print('Test Mean Squared Error = ' + str(testMSE))\n",
        "    print('Learned regression tree model:')\n",
        "    print(model.toDebugString())\n",
        "\n",
        "    # Guarda y carga el modelo\n",
        "    model.save(sc, \"target/tmp/myDecisionTreeRegressionModel\")\n",
        "    sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeRegressionModel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svIriGtL6oBD",
        "outputId": "92fc2e7b-220e-4855-ecb2-5eb0bbaa0ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Mean Squared Error = 0.02857142857142857\n",
            "Learned regression tree model:\n",
            "DecisionTreeModel regressor of depth 1 with 3 nodes\n",
            "  If (feature 434 <= 70.5)\n",
            "   Predict: 0.0\n",
            "  Else (feature 434 > 70.5)\n",
            "   Predict: 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Modelo de clasificación**\n",
        "#### Random Forest"
      ],
      "metadata": {
        "id": "mLLqEvasFdjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.tree import RandomForest, RandomForestModel"
      ],
      "metadata": {
        "id": "YFhml9uJ8WTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí se está entrenando un modelo de Random Forest para clasificación. De igual manera, se carga un conjunto de datos y se divide en conjuntos de entrenamiento y prueba. Se utiliza el algoritmo de Random Forest para crear un modelo de clasificación con ciertos parámetros, como el número de árboles, la estrategia de selección de características, la impureza, la profundidad máxima y el número máximo de divisiones de nodos.\n",
        "\n",
        "Adicionalmente, se realizan predicciones en el conjunto de prueba utilizando el modelo de Random Forest y se calcula la tasa de error en el conjunto de prueba."
      ],
      "metadata": {
        "id": "xB1UnDAPmGTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Carga y analisis del archivo de datos en un RDD de LabeledPoint\n",
        "    data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
        "\n",
        "    # Division de los datos en conjuntos de entrenamiento y prueba\n",
        "    (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "    # Entrenamiento del modelo\n",
        "    model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
        "                                         numTrees=3, featureSubsetStrategy=\"auto\",\n",
        "                                         impurity='gini', maxDepth=4, maxBins=32)\n",
        "\n",
        "    # Evaluacion del modelo en instancias de prueba y calculo del error de prueba\n",
        "    predictions = model.predict(testData.map(lambda x: x.features))\n",
        "    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
        "    testErr = labelsAndPredictions.filter(\n",
        "        lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
        "\n",
        "    print('Test Error = ' + str(testErr))\n",
        "    print('Learned classification forest model:')\n",
        "    print(model.toDebugString())\n",
        "\n",
        "    # Guarda y carga el modelo\n",
        "    model.save(sc, \"target/tmp/myRandomForestClassificationModel\")\n",
        "    sameModel = RandomForestModel.load(sc, \"target/tmp/myRandomForestClassificationModel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rVktrqTCv3H",
        "outputId": "f6e2e060-8b36-4c8b-9c51-091b6f77a950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error = 0.0\n",
            "Learned classification forest model:\n",
            "TreeEnsembleModel classifier with 3 trees\n",
            "\n",
            "  Tree 0:\n",
            "    If (feature 517 <= 56.5)\n",
            "     If (feature 435 <= 39.0)\n",
            "      Predict: 0.0\n",
            "     Else (feature 435 > 39.0)\n",
            "      Predict: 1.0\n",
            "    Else (feature 517 > 56.5)\n",
            "     Predict: 1.0\n",
            "  Tree 1:\n",
            "    If (feature 510 <= 20.5)\n",
            "     If (feature 207 <= 2.0)\n",
            "      Predict: 1.0\n",
            "     Else (feature 207 > 2.0)\n",
            "      If (feature 428 <= 69.0)\n",
            "       Predict: 1.0\n",
            "      Else (feature 428 > 69.0)\n",
            "       Predict: 0.0\n",
            "    Else (feature 510 > 20.5)\n",
            "     Predict: 0.0\n",
            "  Tree 2:\n",
            "    If (feature 323 <= 104.5)\n",
            "     If (feature 460 <= 46.5)\n",
            "      Predict: 0.0\n",
            "     Else (feature 460 > 46.5)\n",
            "      If (feature 268 <= 9.5)\n",
            "       Predict: 1.0\n",
            "      Else (feature 268 > 9.5)\n",
            "       Predict: 0.0\n",
            "    Else (feature 323 > 104.5)\n",
            "     If (feature 496 <= 1.0)\n",
            "      Predict: 1.0\n",
            "     Else (feature 496 > 1.0)\n",
            "      Predict: 0.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Agrupamiento**\n",
        "#### K-means"
      ],
      "metadata": {
        "id": "Q_arxPgQFl27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from math import sqrt\n",
        "from pyspark.mllib.clustering import KMeans, KMeansModel"
      ],
      "metadata": {
        "id": "dLruhFTd9KTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta última sección, se lleva a cabo el agrupamiento de datos utilizando el algoritmo K-means. Los datos se cargan desde un archivo de texto y se analizan. Cada línea del archivo se divide en una matriz de números que representan las características de cada punto de datos. Se construye el modelo K-means y posteriormente se evalúa el agrupamiento calculando la Suma de Errores Cuadrados Dentro del Conjunto (WSSSE), el cual mide la dispersión de los puntos dentro de los clústeres."
      ],
      "metadata": {
        "id": "V7GqcmaCOh7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Carga y analisis de datos\n",
        "    data = sc.textFile('kmeans_data.txt')\n",
        "    parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
        "\n",
        "    # Construccion del modelo\n",
        "    clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
        "\n",
        "    # Evaluacion del agrupamiento calculando la suma de errores cuadrados dentro del conjunto\n",
        "    def error(point):\n",
        "        center = clusters.centers[clusters.predict(point)]\n",
        "        return sqrt(sum([x**2 for x in (point - center)]))\n",
        "\n",
        "    WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
        "    print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
        "\n",
        "    # Guarda y carga el modelo\n",
        "    clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
        "    sameModel = KMeansModel.load(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
        "\n",
        "    # Detiene el contexto existente\n",
        "    sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zwa3Y7S9Sg-",
        "outputId": "12d9878d-9fb4-4fc6-c681-7fada487903e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Within Set Sum of Squared Error = 0.6928203230275529\n"
          ]
        }
      ]
    }
  ]
}